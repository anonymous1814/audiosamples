
<!DOCTYPE html>
<html>
	<head>
		<title>Audio Samples</title>
		<style>
			div {
			margin-bottom: 32px;
			}
			
			.github.td {
				padding-left: 20px;
				padding-right: 20px;
			}

			
			#toc_container {
			background: #f9f9f9 none repeat scroll 0 0;
			border: 1px solid #aaa;
			display: table;
			font-size: 95%;
			padding-top: 20px;
			padding-left: 20px;
			padding-right: 20px;
			padding-bottom: 8px;
			width: auto;
			}
			
			#toc_container ul  {
			  list-style: outside none none !important; margin-left: -40px;
			}
			
			.id {
			width: 245px;
			}
			
			.ood {
			width: 280px;
			}
			
			.first-col {
			padding-right: 20px;
			white-space: nowrap;
			}
				
		  .text {
			font-style: italic;
			color: #666666;
		  }
			
		</style>
	</head>
	
	<body>
		<article>
			<header>
				<h1>VR-TTS: A Lightweight and Rectified Flow Based Text-To-Speech with Variational Alignment Predictor</h1>
			</header>
		</article>
	

		<div>
			<b>Abstract: </b>In recent years, diffusion models such as denoising diffusion probabilistic models and score-based generative models have made significant progress in speech synthesis. However, these models require a large number of sampling steps, which results in relatively high time and computing costs. In this paper, we introduce VR-TTS, an ODE-based Text-to-Speech model leveraging rectified flow. This approach transforms Gaussian distributions into Mel-spectrogram distributions via linear interpolations, achieving high-quality speech synthesis. Additionally, we employ variational alignment predictor to better align input symbols with output frames. The fast linear attention with a single head is also adopted to minimize the number of parameters instead of standard Transformer, which resulting more lightweight model size. Experiments show that our proposed system gets the smallest memory footprint and matches baseline speeds while attaining a comparable mean opinion score in listening tests.
			<p></p>
		</div>

		
		<h2>Contents</h2>
			<div id="toc_container" style="padding-top:0px;">
			<ul>
				<li><a href="#Demo"> 1. Demo
				<li><a href="#Experimental Results"> 2. Experimental Results
				<li><a href="#References"> 3. References
			</ul>
		</div>
		
		<div>
			<p><br></p>

			<style>
				table {
					width: 30%;
					border-collapse: collapse;
					margin: 16px 0;
					margin-left: auto;
            		margin-right: auto;
				}
				thead th {
					border-top: 2px solid black;
					border-bottom: 2px solid black; /* 表头底部边框 */
					padding: 10px;
					text-align: left;
				}
				tbody td {
					border-bottom: 1px solid black; /* 数据行底部边框 */
					padding: 10px;
					text-align: center;
				}
				tfoot td {
					border-top: 2px solid black; /* 表尾顶部边框 */
					padding: 10px;
					text-align: center;
				}
				tfoot {
					border-top: 2px solid black; /* 表格底部边框 */
				}

				.left-align {
					text-align: left; /* 单元格内容左对齐 */
				}
				.center-align {
					text-align: center; /* 单元格内容居中对齐 */
				}
				.right-align {
					text-align: right; /* 单元格内容右对齐 */
				}
			</style>

			<a name="Experimental Results"><h2>Experimental</h2></a>
			<hr>
			<p class="lead" style="text-align: center;">Comparison of  the number of parameters, average RTF, and FLOPs</p>

			<table>
				<thead>
				  <tr>
					<th rowspan="2">Model</th>
					<th colspan="2" class="center-align">Params (M)</th>
					<th colspan="2" class="center-align">RTF</th>
					<th rowspan="2" class="center-align">FLOPs (G)</th>
				  </tr>
				</thead>
				<tbody>
				  <tr>
					<td class="left-align">VITS</td>
					<td>35.30</td>
					<td>28.06</td>
					<td>0.575</td>
					<td>0.0159</td>
					<td>26.22</td>
				  </tr>
				  <tr>
					<td class="left-align">MB-iSTFT-VITS</td>
					<td>34.67</td>
					<td>27.43</td>
					<td>0.053</td>
					<td>0.0072</td>
					<td>8.94</td>
				  </tr>
				  <tr>
					<td class="left-align">EFTS2</td>
					<td>33.04</td>
					<td>24.58</td>
					<td>0.120</td>
					<td>0.0117</td>
					<td>25.84</td>
				  </tr>
				  <tr>
					<td class="left-align">LEF-TTS</td>
					<td>21.86</td>
					<td>13.40</td>
					<td>0.016</td>
					<td>0.0037</td>
					<td>2.65</td>
				  </tr>
				</tbody>
			</table>
		</div>

<p><br></p>
<div class="container">
<a name="References"><h2>References</h2></a>
<div>
  [1] K. Ito, "The LJ speech dataset", <a href="https://keithito.com/LJ-Speech-Dataset/">https://keithito.com/LJ-Speech-Dataset/</a>, 2017.
  <br>
  [2] J. Kim, J. Kong, J. Son, "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech," in <em>Proc. ICML</em>, 2021, pp. 5530-5540.
  <br> 
  [3] Kawamura, Masaya et al, “Lightweight and High-Fidelity End-to-End Text-to-Speech with Multi-Band Generation and Inverse Short-Time Fourier Transform,” in <em>ICASSP</em>, 2023. 
  <br> 
  [4] C. Miao, Q. Zhu, M. Chen et al, “Efficienttts2: Variational End-to-End Text-to-Speech Synthesis And Voice Conversion,” <em>IEEE ACM TASLP</em>, 2024

</div>
</div>
  
		
	</body>
</html>
