
<!DOCTYPE html>
<html>
	<head>
		<title>Audio Samples</title>
		<style>
			div {
			margin-bottom: 32px;
			}
			
			.github.td {
				padding-left: 20px;
				padding-right: 20px;
			}

			
			#toc_container {
			background: #f9f9f9 none repeat scroll 0 0;
			border: 1px solid #aaa;
			display: table;
			font-size: 95%;
			padding-top: 20px;
			padding-left: 20px;
			padding-right: 20px;
			padding-bottom: 8px;
			width: auto;
			}
			
			#toc_container ul  {
			  list-style: outside none none !important; margin-left: -40px;
			}
			
			.id {
			width: 245px;
			}
			
			.ood {
			width: 280px;
			}
			
			.first-col {
			padding-right: 20px;
			white-space: nowrap;
			}
				
		  .text {
			font-style: italic;
			color: #666666;
		  }
			
		</style>
	</head>
	
	<body>
		<article>
			<header>
				<h1>LEF-TTS: Lightweight and Efficient End-to-End Text-to-Speech Synthesis With Multi-Stream Generator</h1>
			</header>
		</article>
	

		<div>
			<b>Abstract: </b>Recently, the field of Text-to-speech synthesis has been predominantly characterized by end-to-end models, with the quality of speech generated by these models becoming increasingly comparable to that of human speech. In this work, we propose a Lightweight and Efficient Text-to-speech model, a fast end-to-end framework based on EfficientTTS 2 with fully differentiable. We utilize Fast Linear Attention with a Single Head instead of the standard stacked Transformer, which decreases computational complexity and reduces parameters. Additionally, we improve a network architecture ConvWaveNet to further decrease model parameters, and accelerate inference through a multi-stream inverse short-time Fourier Transform generator. These improvements significantly reduce model parameters and increase inference speed, thereby achieving the objectives of faster inference and lightweight modeling. Experimental results show that the proposed model achieves speech quality comparable to that of the baseline models, while also offering improved inference speed and reduced model size.
			<p></p>
		</div>

		
		<h2>Contents</h2>
			<div id="toc_container" style="padding-top:0px;">
			<ul>
				<li><a href="#Demo"> 1. Demo
				<li><a href="#Experimental Results"> 2. Experimental Results
				<li><a href="#References"> 3. References
			</ul>
		</div>
		
		<div>
			<p><br></p>

			<style>
				table {
					width: 30%;
					border-collapse: collapse;
					margin: 16px 0;
					margin-left: auto;
            		margin-right: auto;
				}
				thead th {
					border-top: 2px solid black;
					border-bottom: 2px solid black; /* 表头底部边框 */
					padding: 10px;
					text-align: left;
				}
				tbody td {
					border-bottom: 1px solid black; /* 数据行底部边框 */
					padding: 10px;
					text-align: center;
				}
				tfoot td {
					border-top: 2px solid black; /* 表尾顶部边框 */
					padding: 10px;
					text-align: center;
				}
				tfoot {
					border-top: 2px solid black; /* 表格底部边框 */
				}

				.left-align {
					text-align: left; /* 单元格内容左对齐 */
				}
				.center-align {
					text-align: center; /* 单元格内容居中对齐 */
				}
				.right-align {
					text-align: right; /* 单元格内容右对齐 */
				}
			</style>

			<a name="Experimental Results"><h2>Experimental</h2></a>
			<hr>
			<p class="lead" style="text-align: center;">Comparison of  the number of parameters, average RTF on Intel(R) Xeon(R) Gold 6130 CPU @2.10GHz and Tesla V100 GPU, and FLOPs</p>

			<table>
				<thead>
				  <tr>
					<th rowspan="2">Model</th>
					<th colspan="2" class="center-align">#Params (M)</th>
					<th colspan="2" class="center-align">RTF</th>
					<th rowspan="2" class="center-align">FLOPs (G)</th>
				  </tr>
				  <tr>
					<th class="center-align">Total</th>
					<th class="center-align">Infer</th>
					<th class="center-align">CPU</th>
					<th class="center-align">GPU</th>
				  </tr>
				</thead>
				<tbody>
				  <tr>
					<td class="left-align">VITS</td>
					<td>35.30</td>
					<td>28.06</td>
					<td>0.575</td>
					<td>0.0159</td>
					<td>26.22</td>
				  </tr>
				  <tr>
					<td class="left-align">MB-iSTFT-VITS</td>
					<td>34.67</td>
					<td>27.43</td>
					<td>0.053</td>
					<td>0.0072</td>
					<td>8.94</td>
				  </tr>
				  <tr>
					<td class="left-align">EFTS2</td>
					<td>33.04</td>
					<td>24.58</td>
					<td>0.120</td>
					<td>0.0117</td>
					<td>25.84</td>
				  </tr>
				  <tr>
					<td class="left-align">LEF-TTS</td>
					<td>21.86</td>
					<td>13.40</td>
					<td>0.016</td>
					<td>0.0037</td>
					<td>2.65</td>
				  </tr>
				</tbody>
			</table>
		</div>

<p><br></p>
<div class="container">
<a name="References"><h2>References</h2></a>
<div>
  [1] K. Ito, "The LJ speech dataset", <a href="https://keithito.com/LJ-Speech-Dataset/">https://keithito.com/LJ-Speech-Dataset/</a>, 2017.
  <br>
  [2] J. Kim, J. Kong, J. Son, "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech," in <em>Proc. ICML</em>, 2021, pp. 5530-5540.
  <br> 
  [3] Kawamura, Masaya et al, “Lightweight and High-Fidelity End-to-End Text-to-Speech with Multi-Band Generation and Inverse Short-Time Fourier Transform,” in <em>ICASSP</em>, 2023. 
  <br> 
  [4] C. Miao, Q. Zhu, M. Chen et al, “Efficienttts2: Variational End-to-End Text-to-Speech Synthesis And Voice Conversion,” <em>IEEE ACM TASLP</em>, 2024

</div>
</div>
  
		
	</body>
</html>
